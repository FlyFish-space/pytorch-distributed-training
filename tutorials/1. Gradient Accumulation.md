## Gradient Accumulation
### Pytorch手动梯度清零模式的好处
传统的训练函数
```python
model = ...
criterion = ...
optimizer = ...
train_loader = ...
for i,(images,target) in enumerate(train_loader):
    # 1. input output
    images = images.cuda(non_blocking=True)
    target = target.cuda(non_blocking=True)
    outputs = model(images)
    loss = criterion(outputs,target)

    # 2. backward
    optimizer.zero_grad()   # reset gradient
    loss.backward()
    optimizer.step()
```
- 计算损失函数
- `optimizer.zero_grad()`, 清零过往梯度
- `loss.backward()`, 反向传播，计算当前梯度
- `optimizer.step()`, 根据梯度更新网络参数

使用梯度累加计算梯度两种方式: 区别于 `loss function` 函数中 `reduction='mean'` 和 `reduction='sum'` 的不同

- `reduction='mean'`的情况下
```python
model = ...
criterion = ...
optimizer = ...
train_loader = ...
gradient_accumulation_steps = 4
for i,(images,target) in enumerate(train_loader):
    # 1. input output
    images = images.cuda(non_blocking=True)
    target = target.cuda(non_blocking=True)
    outputs = model(images)
    loss = criterion(outputs,target)

    # 2.1 loss regularization
    loss = loss / gradient_accumulation_steps   
    # 2.2 back propagation
    loss.backward()
    # 3. update parameters of net
    if((i+1) % gradient_accumulation_steps)==0:
        # optimizer the net
        optimizer.step()        # update parameters of net
        optimizer.zero_grad()   # reset gradient
```

- `reduction='sum'`的情况下
```python
model = ...
criterion = ...
optimizer = ...
train_loader = ...
gradient_accumulation_steps = 4
for i,(images,target) in enumerate(train_loader):
    # 1. input output
    images = images.cuda(non_blocking=True)
    target = target.cuda(non_blocking=True)
    outputs = model(images)
    loss = criterion(outputs,target)

    # 2.1 不需要进行 regularization 操作
    # loss = loss / gradient_accumulation_steps
    # 2.2 back propagation
    loss.backward()
    # 3. update parameters of net
    if((i+1) % gradient_accumulation_steps)==0:
        # optimizer the net
        optimizer.step()        # update parameters of net
        optimizer.zero_grad()   # reset gradient
```


### Details
#### 1. loss regularization
在`reduction='mean'`下loss对gradient_accumulation_steps进行再平均 
```python
loss = criterion(outputs,target)
loss = loss / gradient_accumulation_steps   
loss.backward()
```
- 因为在`reduction='mean'`的情况下, 每次求出的loss是一个batch内预测和标签误差的平均值，使用梯度累计的时候求出几个batch_size的平均值，进行一次再平均，等效于大batch_size的近似平均

在`reduction='sum'`下不需要进行regularization操作，通过计算可知，不进行regularization操作，和大batch_size的loss等价
### Implemented Work
- [Pytorch为什么需要手动梯度清零](https://www.zhihu.com/question/303070254/answer/573037166)